# Dockerfile for CUDA (NVIDIA GPU) support
# Base image with CUDA 12.1 support
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# Set environment variables early to avoid warnings
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app \
    CUDA_VISIBLE_DEVICES=0 \
    PIP_NO_WARN_SCRIPT_LOCATION=1 \
    PIP_ROOT_USER_ACTION=ignore

# Set working directory
WORKDIR /app

# Install system dependencies in a single layer to reduce image size
# apt-utils prevents debconf warnings
RUN apt-get update && apt-get install -y --no-install-recommends \
    apt-utils \
    python3.11 \
    python3.11-dev \
    python3-pip \
    libglib2.0-0 \
    libgl1-mesa-glx \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    libgtk-3-0 \
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    libv4l-dev \
    libxvidcore-dev \
    libx264-dev \
    libjpeg-dev \
    libpng-dev \
    libtiff-dev \
    libatlas-base-dev \
    libtbb2 \
    libtbb-dev \
    wget \
    curl \
    git \
    && ln -s /usr/bin/python3.11 /usr/bin/python \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Upgrade pip first (must be done before using --root-user-action flag)
# Then install PyTorch with CUDA support
RUN pip install --no-cache-dir --upgrade pip setuptools wheel \
    && pip install --no-cache-dir --root-user-action=ignore \
    torch>=2.0.0 \
    torchvision>=0.15.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Copy requirements file and install dependencies
# Use app-code/ path (matches GitHub repository structure - files are in root)
COPY app-code/requirements-cuda-docker.txt /app/requirements-cuda-docker.txt
RUN pip install --no-cache-dir --root-user-action=ignore -r requirements-cuda-docker.txt

# Ensure directories exist for models and data
RUN mkdir -p /app/models /app/data /app/outputs /app/processed_videos

# Pre-download YOLO models during build (so users don't need to download at runtime)
# Stage 1 uses yolov8n (nano), Stage 2 uses yolov8m (medium)
# This downloads both models and copies them to /app/models/
# Must happen after requirements are installed but before app-code is copied
RUN python -c "import os; os.makedirs('/app/models', exist_ok=True); from ultralytics import YOLO; import shutil; import glob; cache = os.path.expanduser('~/.ultralytics'); print('Downloading YOLOv8n...'); n = YOLO('yolov8n.pt'); files_n = glob.glob(os.path.join(cache, '**/yolov8n.pt'), recursive=True); shutil.copy2(files_n[0], '/app/models/yolov8n.pt') if files_n else None; print('Downloading YOLOv8m...'); m = YOLO('yolov8m.pt'); files_m = glob.glob(os.path.join(cache, '**/yolov8m.pt'), recursive=True); shutil.copy2(files_m[0], '/app/models/yolov8m.pt') if files_m else None"

# Copy ALL application code from app-code directory to /app in container
# This includes: app/, launch_gui.py, setup.py, config/, counting_zone.json, etc.
# The COPY command copies the contents of app-code/ into /app/
COPY app-code/ /app/

# Mark setup as complete so SetupManager doesn't run first-time setup at runtime
RUN touch /app/.setup_complete

# Set CUDA environment variables
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose ports for API and dashboard
EXPOSE 8000 8501

# Health check to verify CUDA is available
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD python -c "import torch; assert torch.cuda.is_available()" || exit 1

# Default command - launch the GUI application
CMD ["python", "launch_gui.py"]

